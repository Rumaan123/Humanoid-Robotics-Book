"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[336],{883(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>a,metadata:()=>r,toc:()=>c});var o=t(4848),s=t(8453);const a={sidebar_position:4},i="Perception and Navigation for Humanoid Robots: Cognitive Systems and Spatial Reasoning",r={id:"module-3/perception-navigation",title:"Perception and Navigation for Humanoid Robots: Cognitive Systems and Spatial Reasoning",description:"Overview",source:"@site/docs/module-3/perception-navigation.md",sourceDirName:"module-3",slug:"/module-3/perception-navigation",permalink:"/Humanoid-Robotics-Book/docs/module-3/perception-navigation",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)",permalink:"/Humanoid-Robotics-Book/docs/module-3/"},next:{title:"NVIDIA Isaac Sim for Photorealistic Simulation: Synthetic Data and Environment Generation",permalink:"/Humanoid-Robotics-Book/docs/module-3/isaac-sim"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction to Humanoid Robot Perception",id:"introduction-to-humanoid-robot-perception",level:2},{value:"Unique Challenges in Humanoid Perception",id:"unique-challenges-in-humanoid-perception",level:3},{value:"Visual Processing for Humanoid Robots",id:"visual-processing-for-humanoid-robots",level:3},{value:"Depth Processing and 3D Understanding",id:"depth-processing-and-3d-understanding",level:2},{value:"Depth-Based Environment Modeling",id:"depth-based-environment-modeling",level:3},{value:"Object Detection and Recognition",id:"object-detection-and-recognition",level:2},{value:"Multi-Modal Object Detection",id:"multi-modal-object-detection",level:3},{value:"Sensor Fusion for Robust Perception",id:"sensor-fusion-for-robust-perception",level:2},{value:"Multi-Sensor Integration",id:"multi-sensor-integration",level:3},{value:"Humanoid Robot Navigation Systems",id:"humanoid-robot-navigation-systems",level:2},{value:"Navigation Architecture for Bipedal Robots",id:"navigation-architecture-for-bipedal-robots",level:3},{value:"Humanoid Motion Control",id:"humanoid-motion-control",level:2},{value:"Bipedal Locomotion Control",id:"bipedal-locomotion-control",level:3},{value:"Integration with ROS 2 Navigation Stack",id:"integration-with-ros-2-navigation-stack",level:2},{value:"Nav2 Integration for Humanoid Robots",id:"nav2-integration-for-humanoid-robots",level:3},{value:"Perception-Action Integration",id:"perception-action-integration",level:2},{value:"Closed-Loop Perception-Action Systems",id:"closed-loop-perception-action-systems",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Real-Time Perception Optimization",id:"real-time-perception-optimization",level:3},{value:"Practical Applications in Humanoid Robotics",id:"practical-applications-in-humanoid-robotics",level:2},{value:"Human-Robot Interaction Navigation",id:"human-robot-interaction-navigation",level:3},{value:"Best Practices for Humanoid Perception and Navigation",id:"best-practices-for-humanoid-perception-and-navigation",level:2},{value:"Safety and Robustness Considerations",id:"safety-and-robustness-considerations",level:3},{value:"Summary",id:"summary",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"perception-and-navigation-for-humanoid-robots-cognitive-systems-and-spatial-reasoning",children:"Perception and Navigation for Humanoid Robots: Cognitive Systems and Spatial Reasoning"}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"Perception and navigation form the cognitive foundation of humanoid robotics, enabling robots to understand their environment and move safely through complex spaces. Unlike wheeled or tracked robots, humanoid robots face unique challenges in perception and navigation due to their complex morphology, dynamic balance requirements, and human-like interaction patterns. This chapter explores the specialized perception and navigation systems designed for humanoid robots, including computer vision algorithms, sensor fusion techniques, and path planning strategies optimized for bipedal locomotion."}),"\n",(0,o.jsx)(n.p,{children:"The integration of perception and navigation systems in humanoid robots requires sophisticated algorithms that can handle the inherent instability of bipedal walking, the need for real-time processing, and the complexity of human-scale environments. These systems must operate reliably in dynamic environments while maintaining the robot's balance and ensuring safe interaction with humans and obstacles."}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this section, you should be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement computer vision algorithms for humanoid robot perception"}),"\n",(0,o.jsx)(n.li,{children:"Design navigation systems suitable for humanoid robot morphology"}),"\n",(0,o.jsx)(n.li,{children:"Integrate sensor fusion for robust environment understanding"}),"\n",(0,o.jsx)(n.li,{children:"Apply path planning algorithms optimized for humanoid locomotion"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"introduction-to-humanoid-robot-perception",children:"Introduction to Humanoid Robot Perception"}),"\n",(0,o.jsx)(n.h3,{id:"unique-challenges-in-humanoid-perception",children:"Unique Challenges in Humanoid Perception"}),"\n",(0,o.jsx)(n.p,{children:"Humanoid robots face distinct perception challenges compared to other robot types:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Dynamic Perspective"}),": The robot's head moves during walking, affecting visual processing"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Balance Constraints"}),": Perception must account for robot's stability during locomotion"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Human-Scale Interactions"}),": Perception optimized for human-sized objects and environments"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multi-Modal Integration"}),": Combining various sensor modalities for robust understanding"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import numpy as np\nimport cv2\nfrom sensor_msgs.msg import Image, PointCloud2\nfrom geometry_msgs.msg import PoseStamped\nfrom nav_msgs.msg import OccupancyGrid\nimport sensor_msgs.point_cloud2 as pc2\n\nclass HumanoidPerceptionSystem:\n    def __init__(self):\n        # Perception components\n        self.visual_processor = VisualProcessor()\n        self.depth_processor = DepthProcessor()\n        self.sensor_fusion = SensorFusionModule()\n        self.object_detector = ObjectDetectionModule()\n        self.human_detector = HumanDetectionModule()\n\n        # State variables\n        self.robot_pose = None\n        self.balance_state = None\n        self.environment_map = None\n\n    def process_perception_data(self, sensor_data):\n        """Process multi-modal sensor data for humanoid perception"""\n        # Process visual data\n        visual_features = self.visual_processor.process(sensor_data[\'image\'])\n\n        # Process depth data\n        depth_features = self.depth_processor.process(sensor_data[\'depth\'])\n\n        # Detect objects\n        objects = self.object_detector.detect(\n            sensor_data[\'image\'],\n            sensor_data[\'depth\']\n        )\n\n        # Detect humans\n        humans = self.human_detector.detect(sensor_data[\'image\'])\n\n        # Fuse sensor data\n        fused_data = self.sensor_fusion.fuse(\n            visual_features,\n            depth_features,\n            objects,\n            humans\n        )\n\n        return fused_data\n\n    def update_environment_map(self, fused_data):\n        """Update environment representation for navigation"""\n        # Create or update occupancy grid\n        self.environment_map = self.create_occupancy_grid(fused_data)\n\n        # Update object positions\n        self.update_object_positions(fused_data)\n\n        return self.environment_map\n'})}),"\n",(0,o.jsx)(n.h3,{id:"visual-processing-for-humanoid-robots",children:"Visual Processing for Humanoid Robots"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class VisualProcessor:\n    def __init__(self):\n        # Feature extraction\n        self.sift = cv2.SIFT_create()\n        self.orb = cv2.ORB_create()\n\n        # Color space processing\n        self.color_spaces = {\n            \'hsv\': cv2.COLOR_BGR2HSV,\n            \'lab\': cv2.COLOR_BGR2LAB,\n            \'gray\': cv2.COLOR_BGR2GRAY\n        }\n\n        # Visual tracking\n        self.tracker = cv2.TrackerKCF_create()\n\n    def process(self, image_msg):\n        """Process visual data from camera"""\n        # Convert ROS image to OpenCV\n        cv_image = self.ros_to_cv2(image_msg)\n\n        # Extract features\n        features = self.extract_features(cv_image)\n\n        # Detect edges and contours\n        edges = self.detect_edges(cv_image)\n        contours = self.find_contours(edges)\n\n        # Color-based segmentation\n        color_segments = self.segment_by_color(cv_image)\n\n        return {\n            \'features\': features,\n            \'edges\': edges,\n            \'contours\': contours,\n            \'color_segments\': color_segments\n        }\n\n    def extract_features(self, image):\n        """Extract visual features using multiple algorithms"""\n        # SIFT features\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        kp_sift, desc_sift = self.sift.detectAndCompute(gray, None)\n\n        # ORB features\n        kp_orb, desc_orb = self.orb.detectAndCompute(gray, None)\n\n        return {\n            \'sift\': {\'keypoints\': kp_sift, \'descriptors\': desc_sift},\n            \'orb\': {\'keypoints\': kp_orb, \'descriptors\': desc_orb}\n        }\n\n    def detect_edges(self, image):\n        """Detect edges using Canny edge detection"""\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n        edges = cv2.Canny(blurred, 50, 150)\n        return edges\n\n    def find_contours(self, edges):\n        """Find contours in edge image"""\n        contours, _ = cv2.findContours(\n            edges,\n            cv2.RETR_EXTERNAL,\n            cv2.CHAIN_APPROX_SIMPLE\n        )\n        return contours\n\n    def segment_by_color(self, image):\n        """Segment image based on color properties"""\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n\n        # Define color ranges (example: red objects)\n        lower_red1 = np.array([0, 50, 50])\n        upper_red1 = np.array([10, 255, 255])\n        lower_red2 = np.array([170, 50, 50])\n        upper_red2 = np.array([180, 255, 255])\n\n        mask1 = cv2.inRange(hsv, lower_red1, upper_red1)\n        mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n        red_mask = mask1 + mask2\n\n        return red_mask\n'})}),"\n",(0,o.jsx)(n.h2,{id:"depth-processing-and-3d-understanding",children:"Depth Processing and 3D Understanding"}),"\n",(0,o.jsx)(n.h3,{id:"depth-based-environment-modeling",children:"Depth-Based Environment Modeling"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class DepthProcessor:\n    def __init__(self):\n        # Depth processing parameters\n        self.min_depth = 0.1  # meters\n        self.max_depth = 10.0  # meters\n        self.depth_threshold = 0.02  # 2cm threshold\n\n    def process(self, depth_msg):\n        """Process depth data for 3D understanding"""\n        # Convert depth message to numpy array\n        depth_image = self.ros_depth_to_numpy(depth_msg)\n\n        # Create point cloud from depth\n        point_cloud = self.depth_to_pointcloud(depth_image)\n\n        # Segment ground plane\n        ground_plane = self.segment_ground_plane(point_cloud)\n\n        # Detect obstacles\n        obstacles = self.detect_obstacles(point_cloud, ground_plane)\n\n        # Calculate surface normals\n        normals = self.calculate_surface_normals(point_cloud)\n\n        return {\n            \'point_cloud\': point_cloud,\n            \'ground_plane\': ground_plane,\n            \'obstacles\': obstacles,\n            \'normals\': normals\n        }\n\n    def depth_to_pointcloud(self, depth_image):\n        """Convert depth image to 3D point cloud"""\n        height, width = depth_image.shape\n\n        # Camera intrinsic parameters (example values)\n        fx, fy = 525.0, 525.0  # focal length\n        cx, cy = 319.5, 239.5  # principal point\n\n        # Create coordinate grids\n        x_coords, y_coords = np.meshgrid(\n            np.arange(width),\n            np.arange(height)\n        )\n\n        # Convert to 3D coordinates\n        x_3d = (x_coords - cx) * depth_image / fx\n        y_3d = (y_coords - cy) * depth_image / fy\n        z_3d = depth_image\n\n        # Stack coordinates\n        points = np.stack([x_3d, y_3d, z_3d], axis=-1)\n\n        # Filter valid points (remove invalid depth values)\n        valid_mask = (depth_image > self.min_depth) & \\\n                    (depth_image < self.max_depth) & \\\n                    ~np.isnan(depth_image)\n\n        valid_points = points[valid_mask]\n\n        return valid_points\n\n    def segment_ground_plane(self, point_cloud):\n        """Segment ground plane using RANSAC algorithm"""\n        from sklearn.linear_model import RANSACRegressor\n\n        if len(point_cloud) < 100:\n            return np.array([]), np.array([])\n\n        # Prepare data for RANSAC\n        X = point_cloud[:, [0, 1]]  # x, y coordinates\n        y = point_cloud[:, 2]       # z coordinate (height)\n\n        # Apply RANSAC\n        ransac = RANSACRegressor(\n            min_samples=50,\n            residual_threshold=0.05,  # 5cm threshold\n            max_trials=1000\n        )\n\n        ransac.fit(X, y)\n\n        # Identify inliers (ground points)\n        inlier_mask = ransac.inlier_mask_\n        ground_points = point_cloud[inlier_mask]\n        obstacle_points = point_cloud[~inlier_mask]\n\n        return ground_points, obstacle_points\n\n    def detect_obstacles(self, point_cloud, ground_plane):\n        """Detect obstacles above ground plane"""\n        ground_points, obstacle_points = ground_plane\n\n        # Group obstacle points into clusters\n        obstacles = self.cluster_obstacles(obstacle_points)\n\n        return obstacles\n\n    def cluster_obstacles(self, points):\n        """Cluster obstacle points into individual obstacles"""\n        from sklearn.cluster import DBSCAN\n\n        if len(points) == 0:\n            return []\n\n        # Use DBSCAN for clustering\n        clustering = DBSCAN(eps=0.2, min_samples=10)  # 20cm eps, 10 min points\n        labels = clustering.fit_predict(points[:, :2])  # Use x,y coordinates\n\n        obstacles = []\n        for label in set(labels):\n            if label == -1:  # Noise points\n                continue\n\n            cluster_points = points[labels == label]\n            obstacle = self.create_obstacle_from_cluster(cluster_points)\n            obstacles.append(obstacle)\n\n        return obstacles\n\n    def create_obstacle_from_cluster(self, points):\n        """Create obstacle representation from point cluster"""\n        # Calculate bounding box\n        min_coords = np.min(points, axis=0)\n        max_coords = np.max(points, axis=0)\n\n        # Calculate center and dimensions\n        center = (min_coords + max_coords) / 2\n        dimensions = max_coords - min_coords\n\n        return {\n            \'center\': center,\n            \'dimensions\': dimensions,\n            \'points\': points,\n            \'type\': self.classify_obstacle_type(dimensions)\n        }\n'})}),"\n",(0,o.jsx)(n.h2,{id:"object-detection-and-recognition",children:"Object Detection and Recognition"}),"\n",(0,o.jsx)(n.h3,{id:"multi-modal-object-detection",children:"Multi-Modal Object Detection"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class ObjectDetectionModule:\n    def __init__(self):\n        # Initialize deep learning models\n        self.detection_model = self.load_detection_model()\n        self.classification_model = self.load_classification_model()\n\n        # Confidence thresholds\n        self.detection_threshold = 0.5\n        self.classification_threshold = 0.7\n\n    def detect(self, image, depth=None):\n        \"\"\"Detect objects in image with optional depth information\"\"\"\n        # Run object detection\n        detections = self.run_detection(image)\n\n        # Filter by confidence\n        confident_detections = [\n            det for det in detections\n            if det['confidence'] > self.detection_threshold\n        ]\n\n        # Add 3D information if depth is available\n        if depth is not None:\n            confident_detections = self.add_3d_information(\n                confident_detections,\n                image,\n                depth\n            )\n\n        # Classify objects\n        classified_detections = self.classify_objects(confident_detections)\n\n        return classified_detections\n\n    def run_detection(self, image):\n        \"\"\"Run object detection on image\"\"\"\n        # This would use a pre-trained model like YOLO, SSD, or similar\n        # For demonstration, we'll simulate detection results\n\n        # Convert image for model input\n        input_tensor = self.preprocess_image(image)\n\n        # Run inference (simulated)\n        # detections = self.detection_model(input_tensor)\n\n        # Simulate detection results\n        detections = [\n            {\n                'bbox': [100, 100, 200, 200],  # [x, y, width, height]\n                'class': 'chair',\n                'confidence': 0.85\n            },\n            {\n                'bbox': [300, 150, 150, 150],\n                'class': 'table',\n                'confidence': 0.92\n            }\n        ]\n\n        return detections\n\n    def add_3d_information(self, detections, image, depth):\n        \"\"\"Add 3D information to detections using depth data\"\"\"\n        for detection in detections:\n            bbox = detection['bbox']\n            x, y, w, h = bbox\n\n            # Extract depth region corresponding to detection\n            depth_region = depth[y:y+h, x:x+w]\n\n            # Calculate distance to object\n            valid_depths = depth_region[depth_region > 0]\n            if len(valid_depths) > 0:\n                avg_distance = np.mean(valid_depths)\n                min_distance = np.min(valid_depths)\n            else:\n                avg_distance = float('inf')\n                min_distance = float('inf')\n\n            # Calculate 3D position (simplified)\n            center_x = x + w // 2\n            center_y = y + h // 2\n            depth_center = depth[center_y, center_x]\n\n            # Convert to 3D coordinates (simplified)\n            # This would use actual camera intrinsics in practice\n            detection['distance'] = avg_distance\n            detection['min_distance'] = min_distance\n            detection['position_3d'] = self.pixel_to_3d(\n                center_x, center_y, depth_center\n            )\n\n        return detections\n\n    def classify_objects(self, detections):\n        \"\"\"Classify detected objects\"\"\"\n        for detection in detections:\n            # This would run a classification model on the detection\n            # For now, we'll just verify the class from detection\n            detection['class_confidence'] = detection['confidence']\n\n        return detections\n\nclass HumanDetectionModule:\n    def __init__(self):\n        # Human detection model\n        self.human_detector = self.load_human_detector()\n        self.pose_estimator = self.load_pose_estimator()\n\n    def detect(self, image):\n        \"\"\"Detect humans in image\"\"\"\n        # Run human detection\n        humans = self.run_human_detection(image)\n\n        # Estimate poses\n        for human in humans:\n            human['pose'] = self.estimate_pose(\n                image,\n                human['bbox']\n            )\n\n        return humans\n\n    def run_human_detection(self, image):\n        \"\"\"Run human detection on image\"\"\"\n        # This would use a human detection model\n        # For demonstration, simulate detection\n        humans = [\n            {\n                'bbox': [50, 100, 80, 200],  # [x, y, width, height]\n                'confidence': 0.95,\n                'distance': self.estimate_distance([50, 100, 80, 200])\n            }\n        ]\n\n        return humans\n\n    def estimate_pose(self, image, bbox):\n        \"\"\"Estimate human pose in bounding box\"\"\"\n        # This would use a pose estimation model\n        # For now, return a simple representation\n        return {\n            'head_position': [bbox[0] + bbox[2]//2, bbox[1]],  # Top center\n            'shoulder_width': bbox[2] * 0.7,\n            'estimated_height': bbox[3]\n        }\n"})}),"\n",(0,o.jsx)(n.h2,{id:"sensor-fusion-for-robust-perception",children:"Sensor Fusion for Robust Perception"}),"\n",(0,o.jsx)(n.h3,{id:"multi-sensor-integration",children:"Multi-Sensor Integration"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class SensorFusionModule:\n    def __init__(self):\n        # Fusion weights for different sensors\n        self.fusion_weights = {\n            'camera': 0.4,\n            'lidar': 0.3,\n            'imu': 0.2,\n            'other': 0.1\n        }\n\n        # Data association\n        self.association_threshold = 0.3  # meters\n\n    def fuse(self, visual_data, depth_data, objects, humans):\n        \"\"\"Fuse data from multiple sensors\"\"\"\n        # Create unified representation\n        fused_environment = {\n            'objects': self.fuse_objects(objects),\n            'humans': self.fuse_humans(humans),\n            'obstacles': self.fuse_obstacles_from_depth(depth_data['obstacles']),\n            'ground_plane': depth_data['ground_plane'],\n            'features': self.fuse_features(visual_data['features'])\n        }\n\n        # Update uncertainty estimates\n        fused_environment = self.calculate_uncertainty(fused_environment)\n\n        return fused_environment\n\n    def fuse_objects(self, objects):\n        \"\"\"Fuse object detections from multiple sources\"\"\"\n        if not objects:\n            return []\n\n        # Group objects by spatial proximity\n        fused_objects = []\n        processed_indices = set()\n\n        for i, obj1 in enumerate(objects):\n            if i in processed_indices:\n                continue\n\n            # Find nearby objects to merge\n            nearby_objects = [obj1]\n            nearby_indices = [i]\n\n            for j, obj2 in enumerate(objects[i+1:], i+1):\n                if j in processed_indices:\n                    continue\n\n                # Calculate distance between objects\n                if self.objects_close(obj1, obj2):\n                    nearby_objects.append(obj2)\n                    nearby_indices.append(j)\n\n            # Merge nearby objects\n            merged_object = self.merge_objects(nearby_objects)\n            fused_objects.append(merged_object)\n\n            # Mark as processed\n            processed_indices.update(nearby_indices)\n\n        return fused_objects\n\n    def objects_close(self, obj1, obj2):\n        \"\"\"Check if two objects are spatially close\"\"\"\n        if 'position_3d' in obj1 and 'position_3d' in obj2:\n            pos1 = np.array(obj1['position_3d'])\n            pos2 = np.array(obj2['position_3d'])\n            distance = np.linalg.norm(pos1 - pos2)\n            return distance < self.association_threshold\n\n        # If no 3D position, use 2D overlap for now\n        return self.bboxes_overlap(obj1['bbox'], obj2['bbox'])\n\n    def merge_objects(self, objects):\n        \"\"\"Merge multiple object detections\"\"\"\n        if len(objects) == 1:\n            return objects[0]\n\n        # Average positions and adjust confidence\n        avg_position = np.mean([\n            obj.get('position_3d', [0, 0, 0])\n            for obj in objects\n        ], axis=0)\n\n        avg_confidence = np.mean([obj['confidence'] for obj in objects])\n\n        # Determine most common class\n        classes = [obj['class'] for obj in objects]\n        most_common_class = max(set(classes), key=classes.count)\n\n        merged_object = objects[0].copy()\n        merged_object['position_3d'] = avg_position\n        merged_object['confidence'] = avg_confidence\n        merged_object['class'] = most_common_class\n        merged_object['fused_count'] = len(objects)\n\n        return merged_object\n\n    def calculate_uncertainty(self, environment):\n        \"\"\"Calculate uncertainty estimates for fused data\"\"\"\n        for obj in environment['objects']:\n            # Uncertainty based on fusion confidence\n            obj['uncertainty'] = 1.0 - obj['confidence']\n\n            # Add distance-based uncertainty\n            if 'distance' in obj:\n                obj['uncertainty'] += min(obj['distance'] * 0.01, 0.3)\n\n        return environment\n"})}),"\n",(0,o.jsx)(n.h2,{id:"humanoid-robot-navigation-systems",children:"Humanoid Robot Navigation Systems"}),"\n",(0,o.jsx)(n.h3,{id:"navigation-architecture-for-bipedal-robots",children:"Navigation Architecture for Bipedal Robots"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import heapq\nfrom geometry_msgs.msg import Pose, Point, Quaternion\nfrom nav_msgs.msg import Path\nfrom std_msgs.msg import Header\n\nclass HumanoidNavigationSystem:\n    def __init__(self):\n        # Navigation components\n        self.global_planner = GlobalPathPlanner()\n        self.local_planner = LocalPathPlanner()\n        self.controller = HumanoidMotionController()\n\n        # State variables\n        self.current_pose = Pose()\n        self.goal_pose = Pose()\n        self.path = Path()\n        self.obstacles = []\n\n        # Navigation parameters\n        self.navigation_state = \'IDLE\'  # IDLE, PLANNING, EXECUTING, RECOVERY\n        self.recovery_behavior = \'CLEAR_COSTMAPS\'\n\n    def navigate_to_goal(self, goal_pose):\n        """Navigate humanoid robot to specified goal"""\n        self.goal_pose = goal_pose\n\n        # Plan global path\n        global_path = self.global_planner.plan(\n            self.current_pose,\n            goal_pose,\n            self.obstacles\n        )\n\n        if not global_path:\n            return False, "Global path planning failed"\n\n        # Execute navigation\n        self.navigation_state = \'EXECUTING\'\n\n        # Follow path with local planning and obstacle avoidance\n        success = self.follow_path(global_path)\n\n        if success:\n            return True, "Navigation completed successfully"\n        else:\n            return False, "Navigation failed"\n\n    def follow_path(self, global_path):\n        """Follow global path with local obstacle avoidance"""\n        path_index = 0\n\n        while path_index < len(global_path.poses) and self.navigation_state == \'EXECUTING\':\n            # Get current goal from global path\n            current_goal = global_path.poses[path_index]\n\n            # Plan local path to current goal\n            local_path = self.local_planner.plan(\n                self.current_pose,\n                current_goal,\n                self.obstacles\n            )\n\n            if not local_path:\n                # Try to recover from local planning failure\n                recovery_success = self.recovery_behavior()\n                if not recovery_success:\n                    return False\n\n            # Execute local path\n            execution_success = self.controller.follow_path(local_path)\n\n            if not execution_success:\n                # Local obstacle avoidance\n                avoidance_path = self.local_planner.avoid_obstacles(\n                    self.current_pose,\n                    current_goal,\n                    self.obstacles\n                )\n\n                if avoidance_path:\n                    execution_success = self.controller.follow_path(avoidance_path)\n\n            if execution_success:\n                path_index += 1\n            else:\n                return False\n\n        return True\n\n    def recovery_behavior(self):\n        """Execute recovery behavior when navigation fails"""\n        if self.recovery_behavior == \'CLEAR_COSTMAPS\':\n            # Clear costmaps to remove temporary obstacles\n            self.clear_costmaps()\n            return True\n        elif self.recovery_behavior == \'ROTATE_IN_PLACE\':\n            # Rotate to find alternative path\n            return self.controller.rotate_in_place()\n        elif self.recovery_behavior == \'BACKUP\':\n            # Back up and try alternative path\n            return self.controller.backup_and_replan()\n\n        return False\n\nclass GlobalPathPlanner:\n    def __init__(self):\n        # Global planner parameters\n        self.planner_type = \'A_STAR\'  # A_STAR, D_STAR, RRT, etc.\n        self.costmap_resolution = 0.05  # 5cm resolution\n        self.planning_frequency = 1.0  # Hz\n\n    def plan(self, start_pose, goal_pose, obstacles):\n        """Plan global path from start to goal"""\n        if self.planner_type == \'A_STAR\':\n            return self.a_star_plan(start_pose, goal_pose, obstacles)\n        elif self.planner_type == \'D_STAR\':\n            return self.d_star_plan(start_pose, goal_pose, obstacles)\n        else:\n            raise ValueError(f"Unknown planner type: {self.planner_type}")\n\n    def a_star_plan(self, start_pose, goal_pose, obstacles):\n        """A* path planning algorithm"""\n        # Convert poses to grid coordinates\n        start_grid = self.pose_to_grid(start_pose)\n        goal_grid = self.pose_to_grid(goal_pose)\n\n        # Create costmap\n        costmap = self.create_costmap(obstacles)\n\n        # Implement A* algorithm\n        open_set = [(0, start_grid)]\n        came_from = {}\n        g_score = {start_grid: 0}\n        f_score = {start_grid: self.heuristic(start_grid, goal_grid)}\n\n        while open_set:\n            current = heapq.heappop(open_set)[1]\n\n            if current == goal_grid:\n                # Reconstruct path\n                path = self.reconstruct_path(came_from, current)\n                return self.grid_path_to_ros_path(path)\n\n            for neighbor in self.get_neighbors(current, costmap):\n                tentative_g_score = g_score[current] + self.distance(current, neighbor)\n\n                if tentative_g_score < g_score.get(neighbor, float(\'inf\')):\n                    came_from[neighbor] = current\n                    g_score[neighbor] = tentative_g_score\n                    f_score[neighbor] = tentative_g_score + self.heuristic(neighbor, goal_grid)\n\n                    heapq.heappush(open_set, (f_score[neighbor], neighbor))\n\n        return None  # No path found\n\n    def create_costmap(self, obstacles):\n        """Create costmap from obstacles"""\n        # This would create a 2D costmap representing the environment\n        # Implementation details omitted for brevity\n        pass\n\n    def heuristic(self, pos1, pos2):\n        """Heuristic function for A* (Euclidean distance)"""\n        return np.sqrt((pos1[0] - pos2[0])**2 + (pos1[1] - pos2[1])**2)\n\n    def get_neighbors(self, pos, costmap):\n        """Get valid neighbors for path planning"""\n        neighbors = []\n        for dx, dy in [(-1,0), (1,0), (0,-1), (0,1), (-1,-1), (-1,1), (1,-1), (1,1)]:\n            neighbor = (pos[0] + dx, pos[1] + dy)\n\n            # Check if neighbor is valid (not in obstacle and within bounds)\n            if self.is_valid_position(neighbor, costmap):\n                neighbors.append(neighbor)\n\n        return neighbors\n\nclass LocalPathPlanner:\n    def __init__(self):\n        # Local planner parameters\n        self.local_radius = 3.0  # meters\n        self.control_frequency = 10.0  # Hz\n        self.max_vel_x = 0.5  # m/s\n        self.max_vel_theta = 1.0  # rad/s\n\n    def plan(self, current_pose, goal_pose, obstacles):\n        """Plan local path with obstacle avoidance"""\n        # Use Dynamic Window Approach (DWA) or similar\n        return self.dwa_plan(current_pose, goal_pose, obstacles)\n\n    def dwa_plan(self, current_pose, goal_pose, obstacles):\n        """Dynamic Window Approach for local path planning"""\n        # Calculate dynamic window\n        vs = self.calculate_velocity_space()\n        vd = self.calculate_dynamic_window(current_pose)\n\n        # Evaluate trajectories\n        best_trajectory = None\n        best_score = float(\'-inf\')\n\n        for vel_x in np.linspace(vd[0], vd[1], 10):  # Linear velocity\n            for vel_theta in np.linspace(vd[2], vd[3], 10):  # Angular velocity\n                trajectory = self.predict_trajectory(\n                    current_pose,\n                    vel_x,\n                    vel_theta\n                )\n\n                score = self.evaluate_trajectory(\n                    trajectory,\n                    goal_pose,\n                    obstacles\n                )\n\n                if score > best_score:\n                    best_score = score\n                    best_trajectory = trajectory\n\n        return best_trajectory\n\n    def calculate_dynamic_window(self, current_pose):\n        """Calculate dynamic window based on robot constraints"""\n        # This would calculate the feasible velocity space\n        # based on robot dynamics and constraints\n        pass\n\n    def predict_trajectory(self, start_pose, vel_x, vel_theta, dt=0.1, steps=10):\n        """Predict trajectory given velocity commands"""\n        trajectory = []\n        current_pose = start_pose\n\n        for _ in range(steps):\n            # Simple kinematic model for prediction\n            new_x = current_pose.position.x + vel_x * dt * np.cos(current_pose.orientation.z)\n            new_y = current_pose.position.y + vel_x * dt * np.sin(current_pose.orientation.z)\n            new_theta = current_pose.orientation.z + vel_theta * dt\n\n            pose = Pose()\n            pose.position.x = new_x\n            pose.position.y = new_y\n            pose.orientation.z = new_theta\n\n            trajectory.append(pose)\n\n            # Update for next step\n            current_pose = pose\n\n        return trajectory\n\n    def evaluate_trajectory(self, trajectory, goal_pose, obstacles):\n        """Evaluate trajectory based on multiple criteria"""\n        # Calculate scores for different criteria\n        goal_score = self.calculate_goal_score(trajectory[-1], goal_pose)\n        obs_score = self.calculate_obstacle_score(trajectory, obstacles)\n        speed_score = self.calculate_speed_score(trajectory)\n\n        # Weighted combination of scores\n        total_score = (0.4 * goal_score +\n                      0.4 * obs_score +\n                      0.2 * speed_score)\n\n        return total_score\n'})}),"\n",(0,o.jsx)(n.h2,{id:"humanoid-motion-control",children:"Humanoid Motion Control"}),"\n",(0,o.jsx)(n.h3,{id:"bipedal-locomotion-control",children:"Bipedal Locomotion Control"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class HumanoidMotionController:\n    def __init__(self):\n        # Control parameters\n        self.step_height = 0.1  # meters\n        self.step_length = 0.3  # meters\n        self.walk_frequency = 1.0  # Hz\n        self.balance_threshold = 0.05  # meters (CoM deviation)\n\n        # Control systems\n        self.balance_controller = BalanceController()\n        self.step_controller = StepController()\n        self.foot_placement_controller = FootPlacementController()\n\n    def follow_path(self, path):\n        """Follow a path with bipedal locomotion"""\n        for i in range(len(path.poses) - 1):\n            current_pose = path.poses[i]\n            next_pose = path.poses[i + 1]\n\n            # Calculate direction and distance to next waypoint\n            dx = next_pose.position.x - current_pose.position.x\n            dy = next_pose.position.y - current_pose.position.y\n            distance = np.sqrt(dx**2 + dy**2)\n            direction = np.arctan2(dy, dx)\n\n            # Walk to next waypoint\n            success = self.walk_to_waypoint(\n                current_pose,\n                next_pose,\n                direction,\n                distance\n            )\n\n            if not success:\n                return False\n\n        return True\n\n    def walk_to_waypoint(self, start_pose, target_pose, direction, distance):\n        """Walk to a specific waypoint"""\n        # Calculate number of steps needed\n        num_steps = int(distance / self.step_length) + 1\n\n        for step in range(num_steps):\n            # Calculate target position for this step\n            step_distance = min(self.step_length, distance - step * self.step_length)\n            target_x = start_pose.position.x + step_distance * np.cos(direction)\n            target_y = start_pose.position.y + step_distance * np.sin(direction)\n\n            target_pose = Pose()\n            target_pose.position.x = target_x\n            target_pose.position.y = target_y\n            target_pose.orientation = start_pose.orientation\n\n            # Execute single step\n            success = self.execute_step(target_pose)\n\n            if not success:\n                return False\n\n        return True\n\n    def execute_step(self, target_pose):\n        """Execute a single walking step"""\n        # Plan step trajectory\n        step_trajectory = self.plan_step_trajectory(target_pose)\n\n        # Execute step while maintaining balance\n        success = self.balance_controller.maintain_balance_during_step(\n            step_trajectory\n        )\n\n        if success:\n            # Update robot\'s actual pose after successful step\n            self.update_robot_pose(target_pose)\n\n        return success\n\nclass BalanceController:\n    def __init__(self):\n        # Balance control parameters\n        self.zmp_controller = ZMPController()\n        self.com_controller = COMController()\n        self.ankle_controller = AnkleController()\n\n    def maintain_balance_during_step(self, step_trajectory):\n        """Maintain balance while executing a step"""\n        # Calculate Zero Moment Point (ZMP) trajectory\n        zmp_trajectory = self.zmp_controller.calculate_zmp_trajectory(\n            step_trajectory\n        )\n\n        # Control Center of Mass (CoM)\n        com_trajectory = self.com_controller.calculate_com_trajectory(\n            zmp_trajectory\n        )\n\n        # Control ankle torques to maintain balance\n        ankle_torques = self.ankle_controller.calculate_ankle_torques(\n            com_trajectory\n        )\n\n        # Execute balance control\n        success = self.execute_balance_control(\n            zmp_trajectory,\n            com_trajectory,\n            ankle_torques\n        )\n\n        return success\n\nclass ZMPController:\n    def __init__(self):\n        # ZMP control parameters\n        self.zmp_margin = 0.05  # 5cm safety margin\n        self.zmp_frequency = 100.0  # Hz control frequency\n\n    def calculate_zmp_trajectory(self, step_trajectory):\n        """Calculate ZMP trajectory for stable walking"""\n        # Implement ZMP-based balance control\n        # This would calculate the desired ZMP trajectory\n        # to maintain balance during the step\n        pass\n\nclass COMController:\n    def __init__(self):\n        # CoM control parameters\n        self.com_height = 0.8  # meters (default CoM height)\n        self.com_frequency = 100.0  # Hz\n\n    def calculate_com_trajectory(self, zmp_trajectory):\n        """Calculate CoM trajectory from ZMP trajectory"""\n        # Use inverted pendulum model to calculate CoM trajectory\n        # from desired ZMP trajectory\n        pass\n\nclass AnkleController:\n    def __init__(self):\n        # Ankle control parameters\n        self.ankle_stiffness = 100.0  # N*m/rad\n        self.ankle_damping = 10.0    # N*m*s/rad\n\n    def calculate_ankle_torques(self, com_trajectory):\n        """Calculate ankle torques for balance"""\n        # Calculate required ankle torques to maintain balance\n        # based on CoM trajectory and current state\n        pass\n'})}),"\n",(0,o.jsx)(n.h2,{id:"integration-with-ros-2-navigation-stack",children:"Integration with ROS 2 Navigation Stack"}),"\n",(0,o.jsx)(n.h3,{id:"nav2-integration-for-humanoid-robots",children:"Nav2 Integration for Humanoid Robots"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom nav2_msgs.action import NavigateToPose\nfrom geometry_msgs.msg import PoseStamped\nfrom sensor_msgs.msg import LaserScan, PointCloud2\nfrom tf2_ros import TransformListener, Buffer\nimport numpy as np\n\nclass HumanoidNav2Interface(Node):\n    def __init__(self):\n        super().__init__(\'humanoid_nav2_interface\')\n\n        # Initialize ROS 2 components\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # Navigation action client\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n\n        # Publishers and subscribers\n        self.odom_sub = self.create_subscription(\n            Odometry,\n            \'/odom\',\n            self.odom_callback,\n            10\n        )\n\n        self.laser_sub = self.create_subscription(\n            LaserScan,\n            \'/scan\',\n            self.laser_callback,\n            10\n        )\n\n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/imu/data\',\n            self.imu_callback,\n            10\n        )\n\n        # State variables\n        self.current_pose = Pose()\n        self.current_twist = Twist()\n        self.laser_data = None\n        self.imu_data = None\n\n    def navigate_to_pose(self, goal_pose):\n        """Navigate to specified pose using Nav2"""\n        # Create navigation goal\n        goal = NavigateToPose.Goal()\n        goal.pose = goal_pose\n\n        # Wait for action server\n        self.nav_client.wait_for_server()\n\n        # Send navigation goal\n        future = self.nav_client.send_goal_async(goal)\n        future.add_done_callback(self.navigation_done_callback)\n\n        return future\n\n    def navigation_done_callback(self, future):\n        """Handle navigation completion"""\n        goal_handle = future.result()\n        if not goal_handle.accepted:\n            self.get_logger().info(\'Navigation goal rejected\')\n            return\n\n        result_future = goal_handle.get_result_async()\n        result_future.add_done_callback(self.navigation_result_callback)\n\n    def navigation_result_callback(self, future):\n        """Handle navigation result"""\n        result = future.result().result\n        self.get_logger().info(f\'Navigation completed: {result}\')\n\n    def odom_callback(self, msg):\n        """Handle odometry data"""\n        self.current_pose = msg.pose.pose\n        self.current_twist = msg.twist.twist\n\n    def laser_callback(self, msg):\n        """Handle laser scan data"""\n        self.laser_data = msg\n        # Process laser data for obstacle detection\n        self.process_laser_obstacles(msg)\n\n    def imu_callback(self, msg):\n        """Handle IMU data"""\n        self.imu_data = msg\n        # Process IMU data for balance estimation\n        self.process_imu_balance(msg)\n\n    def process_laser_obstacles(self, laser_msg):\n        """Process laser scan for obstacle detection"""\n        # Convert laser scan to obstacle information\n        ranges = np.array(laser_msg.ranges)\n        angles = np.linspace(\n            laser_msg.angle_min,\n            laser_msg.angle_max,\n            len(ranges)\n        )\n\n        # Detect obstacles\n        obstacle_ranges = ranges[ranges < 2.0]  # Within 2m\n        obstacle_angles = angles[ranges < 2.0]\n\n        # Create obstacle representation\n        obstacles = []\n        for r, theta in zip(obstacle_ranges, obstacle_angles):\n            if not np.isnan(r) and r > laser_msg.range_min:\n                x = r * np.cos(theta)\n                y = r * np.sin(theta)\n                obstacles.append((x, y))\n\n        return obstacles\n\n    def process_imu_balance(self, imu_msg):\n        """Process IMU data for balance estimation"""\n        # Extract orientation from IMU\n        orientation = imu_msg.orientation\n\n        # Convert quaternion to roll/pitch\n        roll, pitch, yaw = self.quaternion_to_euler(\n            orientation.x,\n            orientation.y,\n            orientation.z,\n            orientation.w\n        )\n\n        # Check balance thresholds\n        max_lean = 0.1  # 5.7 degrees\n        if abs(roll) > max_lean or abs(pitch) > max_lean:\n            self.get_logger().warn(\'Balance threshold exceeded\')\n            # Trigger balance recovery\n            self.trigger_balance_recovery()\n\n    def trigger_balance_recovery(self):\n        """Trigger balance recovery behavior"""\n        # Stop navigation and execute balance recovery\n        self.nav_client.cancel_goal_async()\n        # Implement balance recovery logic\n        pass\n\n    def quaternion_to_euler(self, x, y, z, w):\n        """Convert quaternion to Euler angles"""\n        # Roll (x-axis rotation)\n        sinr_cosp = 2 * (w * x + y * z)\n        cosr_cosp = 1 - 2 * (x * x + y * y)\n        roll = np.arctan2(sinr_cosp, cosr_cosp)\n\n        # Pitch (y-axis rotation)\n        sinp = 2 * (w * y - z * x)\n        if abs(sinp) >= 1:\n            pitch = np.copysign(np.pi / 2, sinp)  # Use 90 degrees if out of range\n        else:\n            pitch = np.arcsin(sinp)\n\n        # Yaw (z-axis rotation)\n        siny_cosp = 2 * (w * z + x * y)\n        cosy_cosp = 1 - 2 * (y * y + z * z)\n        yaw = np.arctan2(siny_cosp, cosy_cosp)\n\n        return roll, pitch, yaw\n'})}),"\n",(0,o.jsx)(n.h2,{id:"perception-action-integration",children:"Perception-Action Integration"}),"\n",(0,o.jsx)(n.h3,{id:"closed-loop-perception-action-systems",children:"Closed-Loop Perception-Action Systems"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class PerceptionActionSystem:\n    def __init__(self):\n        # Perception system\n        self.perception = HumanoidPerceptionSystem()\n\n        # Navigation system\n        self.navigation = HumanoidNavigationSystem()\n\n        # Control system\n        self.controller = HumanoidMotionController()\n\n        # Integration parameters\n        self.perception_frequency = 10.0  # Hz\n        self.control_frequency = 100.0   # Hz\n        self.integration_delay = 0.1     # seconds\n\n        # Action selection\n        self.action_selector = ActionSelector()\n\n    def run_perception_action_loop(self):\n        \"\"\"Run closed-loop perception-action system\"\"\"\n        while True:\n            # Sense environment\n            sensor_data = self.acquire_sensor_data()\n\n            # Process perception\n            environment_state = self.perception.process_perception_data(sensor_data)\n\n            # Update navigation map\n            self.navigation.update_environment_map(environment_state)\n\n            # Select appropriate action\n            action = self.action_selector.select_action(\n                environment_state,\n                self.navigation.get_current_state()\n            )\n\n            # Execute action\n            self.execute_action(action)\n\n            # Wait for next iteration\n            time.sleep(1.0 / self.perception_frequency)\n\n    def acquire_sensor_data(self):\n        \"\"\"Acquire data from all sensors\"\"\"\n        # This would acquire data from ROS topics\n        sensor_data = {\n            'image': self.get_camera_data(),\n            'depth': self.get_depth_data(),\n            'lidar': self.get_lidar_data(),\n            'imu': self.get_imu_data(),\n            'odom': self.get_odom_data()\n        }\n\n        return sensor_data\n\n    def execute_action(self, action):\n        \"\"\"Execute selected action\"\"\"\n        if action['type'] == 'navigate':\n            goal = action['goal']\n            self.navigation.navigate_to_goal(goal)\n        elif action['type'] == 'avoid':\n            direction = action['direction']\n            self.controller.avoid_obstacle(direction)\n        elif action['type'] == 'interact':\n            object_id = action['object_id']\n            self.controller.interact_with_object(object_id)\n        elif action['type'] == 'wait':\n            duration = action['duration']\n            time.sleep(duration)\n\nclass ActionSelector:\n    def __init__(self):\n        # Action selection parameters\n        self.threat_threshold = 0.5  # distance threshold for threats\n        self.goal_bias = 0.8         # bias toward goal-directed actions\n\n    def select_action(self, environment_state, robot_state):\n        \"\"\"Select appropriate action based on environment and robot state\"\"\"\n        # Analyze environment for threats\n        threats = self.identify_threats(environment_state)\n\n        if threats:\n            # Handle threats first\n            return self.select_threat_response(threats, robot_state)\n\n        # Check if goal is reached\n        if self.goal_reached(robot_state):\n            return {'type': 'wait', 'duration': 1.0}\n\n        # Navigate toward goal\n        goal = robot_state.get('goal', None)\n        if goal:\n            return {'type': 'navigate', 'goal': goal}\n\n        # Default: wait\n        return {'type': 'wait', 'duration': 0.1}\n\n    def identify_threats(self, environment_state):\n        \"\"\"Identify potential threats in environment\"\"\"\n        threats = []\n\n        # Check for close obstacles\n        for obj in environment_state.get('objects', []):\n            if obj.get('distance', float('inf')) < self.threat_threshold:\n                threats.append(obj)\n\n        # Check for humans in path\n        for human in environment_state.get('humans', []):\n            if human.get('distance', float('inf')) < self.threat_threshold * 1.5:\n                threats.append(human)\n\n        return threats\n\n    def select_threat_response(self, threats, robot_state):\n        \"\"\"Select response to identified threats\"\"\"\n        # Find closest threat\n        closest_threat = min(threats, key=lambda x: x.get('distance', float('inf')))\n\n        # Determine avoidance direction\n        threat_pos = closest_threat.get('position_3d', [0, 0, 0])\n        robot_pos = robot_state.get('position', [0, 0, 0])\n\n        # Calculate direction to avoid threat\n        avoidance_vector = np.array(robot_pos) - np.array(threat_pos)\n        avoidance_direction = avoidance_vector / np.linalg.norm(avoidance_vector)\n\n        return {\n            'type': 'avoid',\n            'direction': avoidance_direction.tolist()\n        }\n"})}),"\n",(0,o.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsx)(n.h3,{id:"real-time-perception-optimization",children:"Real-Time Perception Optimization"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import threading\nimport queue\nfrom collections import deque\n\nclass OptimizedPerceptionSystem:\n    def __init__(self):\n        # Multi-threaded processing\n        self.processing_threads = []\n        self.data_queue = queue.Queue(maxsize=10)\n        self.result_queue = queue.Queue(maxsize=10)\n\n        # Processing pipelines\n        self.visual_pipeline = VisualProcessingPipeline()\n        self.depth_pipeline = DepthProcessingPipeline()\n        self.fusion_pipeline = FusionPipeline()\n\n        # Performance monitoring\n        self.frame_times = deque(maxlen=100)\n        self.cpu_usage = deque(maxlen=100)\n\n        # Adaptive processing\n        self.processing_quality = \'high\'  # high, medium, low\n        self.target_fps = 30.0\n\n    def start_processing_pipeline(self):\n        """Start multi-threaded processing pipeline"""\n        # Start visual processing thread\n        visual_thread = threading.Thread(\n            target=self.visual_pipeline.run,\n            args=(self.data_queue, self.result_queue)\n        )\n        visual_thread.start()\n        self.processing_threads.append(visual_thread)\n\n        # Start depth processing thread\n        depth_thread = threading.Thread(\n            target=self.depth_pipeline.run,\n            args=(self.data_queue, self.result_queue)\n        )\n        depth_thread.start()\n        self.processing_threads.append(depth_thread)\n\n        # Start fusion thread\n        fusion_thread = threading.Thread(\n            target=self.fusion_pipeline.run,\n            args=(self.result_queue, self.final_output)\n        )\n        fusion_thread.start()\n        self.processing_threads.append(fusion_thread)\n\n    def adaptive_processing(self, current_performance):\n        """Adjust processing quality based on performance"""\n        avg_frame_time = np.mean(list(self.frame_times))\n        target_frame_time = 1.0 / self.target_fps\n\n        if avg_frame_time > target_frame_time * 1.2:  # 20% over target\n            # Reduce processing quality\n            if self.processing_quality == \'high\':\n                self.processing_quality = \'medium\'\n                self.reduce_processing_quality()\n            elif self.processing_quality == \'medium\':\n                self.processing_quality = \'low\'\n                self.reduce_processing_quality()\n        elif avg_frame_time < target_frame_time * 0.8:  # 20% under target\n            # Increase processing quality\n            if self.processing_quality == \'low\':\n                self.processing_quality = \'medium\'\n                self.increase_processing_quality()\n            elif self.processing_quality == \'medium\':\n                self.processing_quality = \'high\'\n                self.increase_processing_quality()\n\n    def reduce_processing_quality(self):\n        """Reduce processing quality to improve performance"""\n        # Reduce image resolution\n        self.visual_pipeline.set_resolution_scale(0.75)\n\n        # Reduce detection confidence thresholds\n        self.visual_pipeline.set_confidence_threshold(0.6)\n\n        # Simplify point cloud processing\n        self.depth_pipeline.set_point_cloud_decimation(0.1)  # 10cm resolution\n\n    def increase_processing_quality(self):\n        """Increase processing quality"""\n        # Increase image resolution\n        self.visual_pipeline.set_resolution_scale(1.0)\n\n        # Increase detection confidence thresholds\n        self.visual_pipeline.set_confidence_threshold(0.7)\n\n        # Improve point cloud processing\n        self.depth_pipeline.set_point_cloud_decimation(0.05)  # 5cm resolution\n\nclass VisualProcessingPipeline:\n    def __init__(self):\n        self.resolution_scale = 1.0\n        self.confidence_threshold = 0.7\n        self.max_objects = 20\n\n    def run(self, input_queue, output_queue):\n        """Run visual processing pipeline"""\n        while True:\n            try:\n                # Get input data\n                data = input_queue.get(timeout=1.0)\n\n                # Process visual data\n                start_time = time.time()\n                result = self.process_frame(data)\n                processing_time = time.time() - start_time\n\n                # Put result in output queue\n                output_queue.put({\n                    \'result\': result,\n                    \'processing_time\': processing_time,\n                    \'timestamp\': time.time()\n                })\n\n            except queue.Empty:\n                continue\n\n    def process_frame(self, frame_data):\n        """Process a single frame"""\n        # Apply resolution scaling\n        if self.resolution_scale < 1.0:\n            new_size = (\n                int(frame_data.shape[1] * self.resolution_scale),\n                int(frame_data.shape[0] * self.resolution_scale)\n            )\n            frame_data = cv2.resize(frame_data, new_size)\n\n        # Run object detection\n        detections = self.run_object_detection(frame_data)\n\n        # Filter by confidence\n        high_conf_detections = [\n            det for det in detections\n            if det[\'confidence\'] > self.confidence_threshold\n        ]\n\n        # Limit number of objects\n        if len(high_conf_detections) > self.max_objects:\n            high_conf_detections = high_conf_detections[:self.max_objects]\n\n        return high_conf_detections\n'})}),"\n",(0,o.jsx)(n.h2,{id:"practical-applications-in-humanoid-robotics",children:"Practical Applications in Humanoid Robotics"}),"\n",(0,o.jsx)(n.h3,{id:"human-robot-interaction-navigation",children:"Human-Robot Interaction Navigation"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class SocialNavigationSystem:\n    def __init__(self):\n        # Social navigation parameters\n        self.personal_space_radius = 1.0  # meters\n        self.social_force_coefficient = 0.5\n        self.group_detection_threshold = 3  # people to form group\n\n        # Human behavior prediction\n        self.behavior_predictor = HumanBehaviorPredictor()\n\n    def navigate_with_social_awareness(self, goal_pose, humans):\n        """Navigate while considering social norms and human behavior"""\n        # Calculate social forces from humans\n        social_forces = self.calculate_social_forces(humans)\n\n        # Predict human movements\n        predicted_human_paths = self.behavior_predictor.predict_paths(humans)\n\n        # Modify navigation to respect social norms\n        modified_goal = self.adjust_goal_for_social_norms(\n            goal_pose,\n            humans,\n            predicted_human_paths\n        )\n\n        # Plan path considering social forces\n        path = self.plan_socially_aware_path(\n            self.current_pose,\n            modified_goal,\n            humans,\n            social_forces\n        )\n\n        return path\n\n    def calculate_social_forces(self, humans):\n        """Calculate repulsive forces from humans"""\n        forces = []\n\n        for human in humans:\n            # Calculate distance to human\n            human_pos = np.array([\n                human[\'position_3d\'][0],\n                human[\'position_3d\'][1]\n            ])\n            robot_pos = np.array([\n                self.current_pose.position.x,\n                self.current_pose.position.y\n            ])\n\n            distance = np.linalg.norm(human_pos - robot_pos)\n\n            if distance < self.personal_space_radius:\n                # Calculate repulsive force\n                direction = robot_pos - human_pos\n                direction = direction / np.linalg.norm(direction)\n\n                # Force decreases with distance\n                force_magnitude = (self.personal_space_radius - distance) * self.social_force_coefficient\n                force = direction * force_magnitude\n\n                forces.append({\n                    \'position\': human_pos,\n                    \'force\': force,\n                    \'magnitude\': force_magnitude\n                })\n\n        return forces\n\n    def adjust_goal_for_social_norms(self, goal, humans, predicted_paths):\n        """Adjust goal to respect social norms"""\n        adjusted_goal = goal\n\n        # Check if goal is in personal space of humans\n        for human in humans:\n            human_pos = np.array([human[\'position_3d\'][0], human[\'position_3d\'][1]])\n            goal_pos = np.array([goal.position.x, goal.position.y])\n\n            distance = np.linalg.norm(human_pos - goal_pos)\n\n            if distance < self.personal_space_radius:\n                # Adjust goal to maintain personal space\n                direction = goal_pos - human_pos\n                direction = direction / np.linalg.norm(direction)\n\n                new_goal_pos = human_pos + direction * self.personal_space_radius\n                adjusted_goal.position.x = new_goal_pos[0]\n                adjusted_goal.position.y = new_goal_pos[1]\n\n        return adjusted_goal\n'})}),"\n",(0,o.jsx)(n.h2,{id:"best-practices-for-humanoid-perception-and-navigation",children:"Best Practices for Humanoid Perception and Navigation"}),"\n",(0,o.jsx)(n.h3,{id:"safety-and-robustness-considerations",children:"Safety and Robustness Considerations"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class SafeNavigationSystem:\n    def __init__(self):\n        # Safety parameters\n        self.safety_margin = 0.5  # meters\n        self.max_navigation_speed = 0.3  # m/s\n        self.emergency_stop_distance = 0.3  # meters\n\n        # Validation systems\n        self.path_validator = PathValidator()\n        self.obstacle_validator = ObstacleValidator()\n\n    def validate_navigation_plan(self, path, environment):\n        """Validate navigation plan for safety"""\n        # Check path clearance\n        if not self.path_validator.check_clearance(path, environment, self.safety_margin):\n            return False, "Path has insufficient clearance"\n\n        # Check for dynamic obstacles\n        if not self.obstacle_validator.check_dynamic_obstacles(path, environment):\n            return False, "Path intersects with dynamic obstacles"\n\n        # Check robot kinematic constraints\n        if not self.check_kinematic_feasibility(path):\n            return False, "Path violates kinematic constraints"\n\n        return True, "Path is safe"\n\n    def check_kinematic_feasibility(self, path):\n        """Check if path is kinematically feasible for humanoid"""\n        # Check step length constraints\n        for i in range(len(path.poses) - 1):\n            pose1 = path.poses[i]\n            pose2 = path.poses[i + 1]\n\n            step_distance = np.sqrt(\n                (pose2.position.x - pose1.position.x)**2 +\n                (pose2.position.y - pose1.position.y)**2\n            )\n\n            if step_distance > self.max_step_length:\n                return False\n\n        return True\n\n    def emergency_stop(self):\n        """Execute emergency stop procedure"""\n        # Stop all motion immediately\n        self.controller.stop_motion()\n\n        # Clear navigation goals\n        self.navigation.cancel_all_goals()\n\n        # Enter safe posture\n        self.controller.enter_safe_posture()\n\n        # Log emergency event\n        self.log_emergency_event()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"Perception and navigation systems form the cognitive foundation of humanoid robotics, enabling robots to understand their environment and move safely through complex spaces. The integration of computer vision, depth processing, sensor fusion, and navigation algorithms creates sophisticated systems that can handle the unique challenges of bipedal locomotion and human-scale interaction."}),"\n",(0,o.jsx)(n.p,{children:"Successful implementation requires careful consideration of real-time performance, safety constraints, and the dynamic nature of human environments. The closed-loop integration of perception and action systems enables humanoid robots to adapt to changing conditions while maintaining stability and achieving navigation goals."}),"\n",(0,o.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'"Principles of Robot Motion" by Howie Choset et al.'}),"\n",(0,o.jsx)(n.li,{children:'"Probabilistic Robotics" by Sebastian Thrun, Wolfram Burgard, and Dieter Fox'}),"\n",(0,o.jsx)(n.li,{children:'"Humanoid Robotics: A Reference" by Ambarish Goswami and Prahlad Vadakkepat'}),"\n",(0,o.jsx)(n.li,{children:'"Robotics, Vision and Control" by Peter Corke'}),"\n",(0,o.jsxs)(n.li,{children:["ROS Navigation Stack Documentation: ",(0,o.jsx)(n.a,{href:"http://wiki.ros.org/navigation",children:"http://wiki.ros.org/navigation"})]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>i,x:()=>r});var o=t(6540);const s={},a=o.createContext(s);function i(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);