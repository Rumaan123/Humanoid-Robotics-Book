"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics_book=globalThis.webpackChunkphysical_ai_humanoid_robotics_book||[]).push([[267],{4098(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>s,metadata:()=>r,toc:()=>c});var t=i(4848),o=i(8453);const s={sidebar_position:4},a="Module 4: Vision-Language-Action (VLA)",r={id:"module-4/index",title:"Module 4: Vision-Language-Action (VLA)",description:"Overview",source:"@site/docs/module-4/index.md",sourceDirName:"module-4",slug:"/module-4/",permalink:"/Humanoid-Robotics-Book/docs/module-4/",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"Sim-to-Real Transfer Concepts and Constraints: Bridging the Reality Gap",permalink:"/Humanoid-Robotics-Book/docs/module-3/sim-to-real"},next:{title:"Integration of LLMs with Robotics Systems",permalink:"/Humanoid-Robotics-Book/docs/module-4/llm-integration"}},l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Module Structure",id:"module-structure",level:2},{value:"Topics Covered",id:"topics-covered",level:3},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Real-World Applications",id:"real-world-applications",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsx)(n.p,{children:"Module 4 introduces the Vision-Language-Action (VLA) paradigm, representing the cognitive layer of Physical AI systems. This module explores how large language models (LLMs) integrate with robotics systems to enable natural human-robot interaction, cognitive planning, and multi-modal understanding. Students will learn how to bridge the gap between high-level natural language commands and low-level robot actions through sophisticated AI pipelines."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this module, students will be able to:"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"FR-016: LLM Integration"}),": Understand how large language models integrate with robotics systems and implement appropriate architectural patterns for real-time robotic applications."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"FR-017: Voice-to-Action Pipelines"}),": Design and implement voice-to-action pipelines using speech recognition and natural language processing for natural human-robot interaction."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"FR-018: Cognitive Planning"}),": Implement cognitive planning systems that translate natural language commands to executable ROS 2 action sequences with appropriate safety and validation measures."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"FR-019: Multi-Modal Interaction"}),": Create integrated multi-modal interaction systems that combine vision, speech, and motion for natural and intuitive human-robot communication."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"FR-020: Autonomous Execution"}),": Apply Vision-Language-Action principles to develop complete autonomous humanoid task execution capabilities that demonstrate the integration of all Physical AI concepts."]}),"\n",(0,t.jsx)(n.h2,{id:"module-structure",children:"Module Structure"}),"\n",(0,t.jsx)(n.p,{children:"This module builds upon the foundational knowledge from previous modules to create intelligent, responsive robotic systems capable of understanding and executing complex natural language commands. The content progresses from LLM integration fundamentals to complete VLA system implementation."}),"\n",(0,t.jsx)(n.h3,{id:"topics-covered",children:"Topics Covered"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"./llm-integration",children:"Integration of LLMs with Robotics Systems"})})," - Understanding the architectural patterns for connecting large language models with robotic platforms, including real-time inference considerations and safety protocols."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"./voice-action-pipelines",children:"Voice-to-Action Pipelines"})})," - Developing speech recognition systems that transform spoken commands into executable robot behaviors, incorporating noise filtering and intent recognition."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"./cognitive-planning",children:"Cognitive Planning from Natural Language"})})," - Creating AI systems that parse natural language commands and generate sequences of ROS 2 actions to accomplish user intents."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"./multi-modal",children:"Multi-Modal Interaction"})})," - Integrating vision, speech, and motion into cohesive interaction systems that enable rich human-robot communication and collaboration."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"./capstone-overview",children:"Capstone: Autonomous Humanoid Task Execution"})})," - Synthesizing all learned concepts into a complete VLA system capable of executing complex humanoid tasks through natural language commands."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsx)(n.p,{children:"Students should have a solid understanding of:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"ROS 2 middleware concepts (covered in Module 1)"}),"\n",(0,t.jsx)(n.li,{children:"Digital twin simulation environments (covered in Module 2)"}),"\n",(0,t.jsx)(n.li,{children:"Perception and navigation systems (covered in Module 3)"}),"\n",(0,t.jsx)(n.li,{children:"Basic knowledge of machine learning and neural networks"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"real-world-applications",children:"Real-World Applications"}),"\n",(0,t.jsx)(n.p,{children:"This module addresses FR-016 through FR-020, focusing on the integration of cognitive AI capabilities with physical robotic systems. The concepts covered enable the development of robots that can understand and respond to natural human communication, bridging the gap between human intention and robotic action."}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"After completing this module, students will have mastered the complete Physical AI pipeline from ROS 2 middleware to simulation environments, perception systems, and finally cognitive interaction layers. The capstone project integrates all four modules to demonstrate a fully autonomous humanoid system."})]})}function u(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>a,x:()=>r});var t=i(6540);const o={},s=t.createContext(o);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);